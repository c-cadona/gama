{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/c-cadona/gama/blob/main/Gama.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3fenOLUMt-n"
      },
      "source": [
        "# Readme\n",
        "\n",
        "**Reprodução do Artigo - Previsão de Irradiância**\n",
        "\n",
        "Este notebook tem o objetivo de reproduzir os experimentos descritos no artigo que o Lorenzo me enviou utilizando o dataset Folsom. O foco será obter resultados comparáveis ao artigo, considerando diferentes condições climáticas:\n",
        "\n",
        "- All: Todas as imagens do dataset\n",
        "- Clear: Apenas imagens de céu limpo --> **Apenas esse por enquanto.**\n",
        "- Cloudy: Apenas imagens nubladas\n",
        "\n",
        "As métricas de avaliação utilizadas serão:\n",
        "- RMSE (Erro Quadrático Médio)\n",
        "- MAE (Erro Médio Absoluto)\n",
        "- R² (Coeficiente de Determinação)\n",
        "- Cross-Validation (KFold)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7H9IksWM4xq"
      },
      "source": [
        "## 1. Preparando ambiente\n",
        "\n",
        "- Instalação de bibliotecas\n",
        "- Cuda"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HothzHzfNCsk"
      },
      "source": [
        "### 1.1 Bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "RvCQo7h8NCXo",
        "outputId": "a1190cab-e492-42b5-fa5f-00691b5d9546"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[91m━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.7/664.8 MB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:24\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Collecting numpy\n",
            "  Downloading numpy-2.2.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.14.1)\n",
            "Collecting scipy\n",
            "  Downloading scipy-1.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Collecting pandas\n",
            "  Using cached pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Collecting matplotlib\n",
            "  Using cached matplotlib-3.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading numpy-2.2.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m65.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.6/37.6 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "Using cached matplotlib-3.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
            "Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, scipy, pandas, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, matplotlib\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.14.1\n",
            "    Uninstalling scipy-1.14.1:\n",
            "      Successfully uninstalled scipy-1.14.1\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.10.0\n",
            "    Uninstalling matplotlib-3.10.0:\n",
            "      Successfully uninstalled matplotlib-3.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.4 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed matplotlib-3.10.1 numpy-2.2.4 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pandas-2.2.3 scipy-1.15.2\n"
          ]
        }
      ],
      "source": [
        "!pip install -U pandas matplotlib torch torchvision scikit-learn --quiet\n",
        "!pip install --upgrade numpy scipy pandas matplotlib scikit-learn torch torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "cpD_5ovqMpju"
      },
      "outputs": [],
      "source": [
        "# Importar PyTorch\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Importar bibliotecas adicionais\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hkcZmMEMtUK"
      },
      "source": [
        "### 1.2 Cuda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bL93UV8CTCGD",
        "outputId": "29bee6f3-331a-4ac9-e041-4ff0e167eb56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  torch.manual_seed(42)\n",
        "\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R89vFqcdNSwU"
      },
      "source": [
        "## 2. Carregando os dados\n",
        "\n",
        "- Acessar dados no drive\n",
        "- Carregar os dados\n",
        "- Organizar os dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_V-9hLbFNZGE"
      },
      "source": [
        "## 2.1 Acessando dados do drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "QwOxiBwjNX7y",
        "outputId": "aa059984-539d-4f99-ea5b-328150807bba"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "mount failed",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-7c32e87e2c85>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mbase_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/Task - GAMMA\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         )\n\u001b[0;32m--> 277\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: mount failed"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "base_path = \"/content/drive/MyDrive/Task - GAMMA\"\n",
        "\n",
        "# Verifica se já existe o link simbólico e remove antes de recriar\n",
        "if os.path.exists(\"folsom_dataset\") or os.path.islink(\"folsom_dataset\"):\n",
        "  os.unlink(\"folsom_dataset\")  # Remove o link simbólico ou diretório existente\n",
        "\n",
        "# Criar link simbólico para facilitar o acesso\n",
        "os.symlink(os.path.join(base_path, \"folsom_dataset\"), \"folsom_dataset\")\n",
        "print(\"Symbolic link created.\")\n",
        "\n",
        "print(\"Arquivos na pasta folsom_dataset:\", os.listdir(\"folsom_dataset\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbEPEemmNcCH"
      },
      "source": [
        "### 2.2 Extraindo dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wWx-QJXyNed2"
      },
      "outputs": [],
      "source": [
        "# unzippar as imagens dentro da VM do colab, para nao ocupar disco do drive\n",
        "!unzip folsom_dataset/folsom_images.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlBTbDVmNgIJ"
      },
      "source": [
        "### 2.3 Organizando dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "OtS14FrfNhyD"
      },
      "outputs": [],
      "source": [
        "# date modified das imagens\n",
        "df_date_modif = pd.read_csv(\"folsom_dataset/df_date_modif.csv\", index_col=0, parse_dates=True)\n",
        "df_date_modif"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "tPlIsaKQNjVB"
      },
      "outputs": [],
      "source": [
        "# dados de irradiance\n",
        "df_irradiance = pd.read_csv(\"folsom_dataset/Folsom_irradiance.csv\", index_col=0, parse_dates=True)\n",
        "df_irradiance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "BriI7kWhNlN6"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# timestamps do conjunto de teste utilizado, LEVANDO EM CONSIDERAÇÃO O DATE MODIFIED COMO DATA REAL DAS IMAGENS\n",
        "with open(\"folsom_dataset/test_timestamps.pkl\", \"rb\") as f:\n",
        "  test_timestamps = pd.to_datetime(pickle.load(f))\n",
        "test_timestamps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "zfJkprLtNmaP"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
        "ax.imshow(np.load(df_date_modif[\"path\"].iloc[12345]))\n",
        "ax.set_axis_off()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bhl6hmJANnXl"
      },
      "outputs": [],
      "source": [
        "# hparams de treinamento\n",
        "import json\n",
        "\n",
        "with open(\"folsom_dataset/hparams.json\", \"r\") as f:\n",
        "  hparams = json.load(f)\n",
        "\n",
        "batch_size = hparams[\"ResNet50\"][\"batch_size\"]\n",
        "learning_rate = hparams[\"ResNet50\"][\"learning_rate\"]\n",
        "dropout = hparams[\"ResNet50\"][\"dropout\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoTEbfMINoE7"
      },
      "source": [
        "## 3. Categorização - Sunmask\n",
        "\n",
        "Utilizarei o pvlib - https://pvlib-python.readthedocs.io/en/stable/ para categorizar os dados e dividi-los como: Clear - Cloudy - All\n",
        "\n",
        "**OBS**: Por enquanto utilizarei o csv que o Lorenzo me passou com os timestamps do céu limpo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "UGWaVBLRNsHd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_clear_sky = pd.read_csv(\"folsom_dataset/df_clear_sky.csv\", index_col=0, parse_dates=True)\n",
        "\n",
        "# Filtrando os dataframes para utilizar apenas céu limpo\n",
        "df_date_modif_clear = df_date_modif[df_date_modif.index.isin(df_clear_sky.index)]\n",
        "df_irradiance_clear = df_irradiance[df_irradiance.index.isin(df_clear_sky.index)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "34sZT4mhVJoj"
      },
      "outputs": [],
      "source": [
        "# Instalando pvlib\n",
        "# !pip uninstall -y numpy pvlib\n",
        "# !pip install numpy\n",
        "# !pip install --no-cache-dir pvlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7xMSfxlc8wt"
      },
      "source": [
        "## 4. Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEBrf02_dA2F"
      },
      "source": [
        "### 4.1 Transform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "GwSTl8cwXRSw"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "transform = transforms.Compose([\n",
        "  # transforms.Resize((64, 64)),\n",
        "  transforms.ToTensor(),\n",
        "  transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSOkl-3ccl0k"
      },
      "source": [
        "### 4.2 Classe Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "08xbfbnXdFD9"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "\n",
        "class FolsomDataset(Dataset):\n",
        "  def __init__(self, df_date_modif, df_irradiance, transform=None):\n",
        "    \"\"\"\n",
        "      Args:\n",
        "      df_date_modif (DataFrame): Contém os caminhos das imagens.\n",
        "      df_irradiance (DataFrame): Contém os valores de irradiância indexados pelo timestamp.\n",
        "      transform (callable, optional): Transformações a serem aplicadas às imagens.\n",
        "      \"\"\"\n",
        "    self.df_date_modif = df_date_modif\n",
        "    self.df_irradiance = df_irradiance\n",
        "    self.transform = transform\n",
        "    self.data = list(zip(df_date_modif[\"path\"], df_irradiance[\"ghi\"]))\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    image_path, irradiance_value = self.data[idx]\n",
        "    image_data = np.load(image_path)\n",
        "    image = Image.fromarray(image_data.astype(np.uint8))\n",
        "\n",
        "    if self.transform:\n",
        "      image = self.transform(image)\n",
        "\n",
        "    irradiance_value = torch.tensor(irradiance_value, dtype=torch.float32)\n",
        "    return image, irradiance_value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AdcAbrzdRGe"
      },
      "source": [
        "### 4.3 DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "rC7Oo6F1dZCw"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Subset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Criando os datasets (treino e teste)\n",
        "train_dataset = FolsomDataset(df_date_modif=df_date_modif_clear, df_irradiance=df_irradiance_clear, transform=transform)\n",
        "test_dataset = FolsomDataset(df_date_modif=df_date_modif_clear, df_irradiance=df_irradiance_clear, transform=transform)\n",
        "\n",
        "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "# test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,\n",
        "# num_workers=2)\n",
        "\n",
        "\"\"\"\n",
        "Criando sub sets:\n",
        "- Treino: 80% dos dados (dias ímpares)\n",
        "- Teste: 20% dos dados (dias pares)\n",
        "- Validação: 10% dos dados (dias ímpares sem interseção com o treino)\n",
        "\"\"\"\n",
        "\n",
        "df_date_modif['day_of_year'] = df_date_modif['timestamp'].dt.dayofyear\n",
        "\n",
        "train_idx = df_date_modif[df_date_modif['day_of_year'] % 2 == 1].index\n",
        "test_idx = df_date_modif[df_date_modif['day_of_year'] % 2 == 0].index\n",
        "\n",
        "train_indices, val_indices = train_test_split(\n",
        "  train_indices,\n",
        "  test_size=0.20,\n",
        "  random_state=42,\n",
        "  shuffle=True\n",
        ")\n",
        "\n",
        "# Criando sub sets\n",
        "train_subset = Subset(train_dataset, train_indices.tolist())\n",
        "val_subset = Subset(train_dataset, val_indices.tolist())\n",
        "test_subset = Subset(test_dataset, test_indices.tolist())\n",
        "\n",
        "# Criando os DataLoaders\n",
        "train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "test_loader = DataLoader(test_subset, batch_size=batch_size, shuffle=False, num_workers=2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-5JgvzHdL5v"
      },
      "source": [
        "## 5. Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "oJwDvz2zdQrt"
      },
      "outputs": [],
      "source": [
        "resnet50 = models.resnet50(weights=\"DEFAULT\")\n",
        "\n",
        "resnet50.fc = nn.Sequential(\n",
        "  nn.Dropout(hparams[\"ResNet50\"][\"dropout\"]),\n",
        "  nn.Linear(resnet50.fc.in_features, 1)\n",
        ")\n",
        "\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "optimizer = optim.AdamW(resnet50.parameters(), lr=hparams[\"ResNet50\"][\"learning_rate\"], weight_decay=hparams[\"ResNet50\"][\"weight_decay\"])\n",
        "\n",
        "gamma=(1/10) ** (1/(0.75 * hparams[\"ResNet50\"][\"epochs\"]))\n",
        "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma)\n",
        "\n",
        "# init_lr = hyperparameters[\"learning_rate\"] #e-3\n",
        "# last_lr = hyperparameters[\"learning_rate\"] / 10 #e-4\n",
        "# n_epochs = int(hyperparameters[\"epochs\"] * hyperparameters[\"init_decay_epochs\"])\n",
        "# gamma = np.exp(np.log(last_lr / init_lr) / n_epochs)\n",
        "\n",
        "model = resnet50.to(device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HffzEJSAdj1g"
      },
      "source": [
        "## 6. Treinamento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGAJzMRidrfg"
      },
      "source": [
        "### 6.1 Loop de treinamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "AhGDdyhgdmxo"
      },
      "outputs": [],
      "source": [
        "epochs = hparams[\"ResNet50\"][\"epochs\"]\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  model.train()\n",
        "  running_loss = 0.0\n",
        "\n",
        "  for images, targets in train_loader:\n",
        "    images, targets = images.to(device), targets.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    outputs = resnet50(images).squeeze()\n",
        "    loss = loss_fn(outputs, targets)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss += loss.item()\n",
        "\n",
        "  scheduler.step()\n",
        "\n",
        "  print(f\"Epoch {epoch+1}/{epochs}, Loss: {(running_loss/len(train_loader)):.4f}, LR: {optimizer.param_groups[0]['lr']:.15f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHdIY1bvdpC0"
      },
      "source": [
        "### 6.2 Fazendo predições"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "mni_d7d6dunD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed_all(42)\n",
        "\n",
        "def predict(model, dataloader, device):\n",
        "  model.eval()\n",
        "  predictions, ground_truths = [], []\n",
        "\n",
        "  with torch.inference_mode():\n",
        "    for images, targets in dataloader:\n",
        "      images, targets = images.to(device), targets.to(device)\n",
        "      outputs = model(images).squeeze()\n",
        "\n",
        "      predictions.extend(outputs.cpu().numpy())\n",
        "      ground_truths.extend(targets.cpu().numpy())\n",
        "\n",
        "  return np.array(predictions), np.array(ground_truths)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUkcm8i4dy1O"
      },
      "source": [
        "## 7. Avaliando modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "IxSSDMykeQI3"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade numpy scipy torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "mm17Roh3d0Df"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def evaluate_model(predictions, ground_truths):\n",
        "    predictions = torch.tensor(predictions, dtype=torch.float32)\n",
        "    ground_truths = torch.tensor(ground_truths, dtype=torch.float32)\n",
        "\n",
        "    # Erro quadrático médio (MSE) e RMSE\n",
        "    mse = torch.mean((predictions - ground_truths) ** 2)\n",
        "    rmse = torch.sqrt(mse)\n",
        "\n",
        "    # Erro absoluto médio (MAE)\n",
        "    mae = torch.mean(torch.abs(predictions - ground_truths))\n",
        "\n",
        "    # Desvio padrão dos erros (RMSE e MAE)\n",
        "    std_rmse = torch.std((predictions - ground_truths) ** 2).sqrt()\n",
        "    std_mae = torch.std(torch.abs(predictions - ground_truths))\n",
        "\n",
        "    # R² Score (coeficiente de determinação)\n",
        "    ss_res = torch.sum((ground_truths - predictions) ** 2)\n",
        "    ss_tot = torch.sum((ground_truths - torch.mean(ground_truths)) ** 2)\n",
        "    r2 = 1 - ss_res / ss_tot if ss_tot != 0 else torch.tensor(0.0)\n",
        "\n",
        "    return rmse.item(), std_rmse.item(), mae.item(), std_mae.item(), r2.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XHfUXTHo6gB3"
      },
      "outputs": [],
      "source": [
        "predictions, ground_truths = predict(model, test_loader, device)\n",
        "rmse, std_rmse, mae, std_mae, r2 = evaluate_model(predictions, ground_truths)\n",
        "\n",
        "print(f\"RMSE: {rmse:.2f} ± {std_rmse:.2f} W/m²\")\n",
        "print(f\"MAE: {mae:.2f} ± {std_mae:.2f} W/m²\")\n",
        "print(f\"R²: {r2}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRW99vJOzN_k"
      },
      "source": [
        "Gabarito:\n",
        "\n",
        "- RMSE: 12.89 ± 0.39 W/m²\n",
        "- MAE: 9.80 ± 0.23 W/m²"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3CJo5LAozWjJ"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def plot_predictions_vs_truth(predictions, ground_truths):\n",
        "    plt.figure(figsize=(9, 9))\n",
        "    plt.scatter(ground_truths, predictions, alpha=0.5, color='royalblue', edgecolors='k')\n",
        "    plt.plot([min(ground_truths), max(ground_truths)],\n",
        "             [min(ground_truths), max(ground_truths)],\n",
        "             color='red', linestyle='--', label='Ideal (y = x)')\n",
        "\n",
        "    plt.xlabel(\"Valor Real (W/m²)\")\n",
        "    plt.ylabel(\"Predição do Modelo (W/m²)\")\n",
        "    plt.title(\"Predições vs Valores Reais\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.axis('equal')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "geyL3edf90EY"
      },
      "outputs": [],
      "source": [
        "plot_predictions_vs_truth(predictions, ground_truths)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rl1HqdLrJ-7C"
      },
      "source": [
        "## 8. Salvando resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8Qfyxdlf92eR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "def salvar_resultados(predictions, ground_truths, rmse, std_rmse, mae, std_mae, r2, caminho=\"resultados.csv\"):\n",
        "    # Organizar dados\n",
        "    data = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    pred_str = \";\".join([f\"{p:.4f}\" for p in predictions])\n",
        "    gt_str = \";\".join([f\"{g:.4f}\" for g in ground_truths])\n",
        "\n",
        "    nova_linha = {\n",
        "        \"Data\": data,\n",
        "        \"Predictions\": pred_str,\n",
        "        \"GroundTruths\": gt_str,\n",
        "        \"RMSE\": rmse,\n",
        "        \"STD_RMSE\": std_rmse,\n",
        "        \"MAE\": mae,\n",
        "        \"STD_MAE\": std_mae,\n",
        "        \"R2\": r2\n",
        "    }\n",
        "\n",
        "    # Verifica se o arquivo existe\n",
        "    if os.path.exists(caminho):\n",
        "        df_existente = pd.read_csv(caminho)\n",
        "        df_novo = pd.concat([df_existente, pd.DataFrame([nova_linha])], ignore_index=True)\n",
        "    else:\n",
        "        df_novo = pd.DataFrame([nova_linha])\n",
        "\n",
        "    # Salva\n",
        "    df_novo.to_csv(caminho, index=False)\n",
        "    print(f\"Resultados salvos com sucesso em '{caminho}'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "iSLS9uycKtsb"
      },
      "outputs": [],
      "source": [
        "predictions, ground_truths = predict(model, test_loader, device)\n",
        "\n",
        "salvar_resultados(predictions, ground_truths, rmse, std_rmse, mae, std_mae, r2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "t4orFO6OK7Le"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyMpzhTHle8Zu/SByIaJ3kXt",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}